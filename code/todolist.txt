

# clean apartment
# taxes  
# check flight reimbursement 

todo list (deadline in 3 weeks, citymap deadline in 2 days):

	3/11
		make real taxi dataset environment real taxi datasets
			- make map stuff and utility functions inside city map environment 
			- make plotting debugging for chicago map 
			- start running it 
		profile code 
		diagnose long blll runtime - why no convergence? 
		distributed auction task assignment ??

	app token? 
	refactor gridworld utilities that you changed in citymap 
	it looks like agents are avoiding value function?? 
	review proof
	make fig 1 
	nice to have
	- how to encorporate traffic ?? 
	
	develop uniform baseline

	hardcore debug 
		- is ncells correct in current gridworld formulation? shouldnt it be (nx-1)*(ny-1) bc there are less cells than grid intersections (bc of endpoints) 

	refactoring
		- put render in plotter 
		- clean 

	experiments
		- micro: 
			- exp1. cumulative reward for different algorithms with standard deviation plots  
			- exp2. gif of evolving simulation - > data generating 
			- largescale sim -> one trial one algorithm, 1000 agents
		- macro: 
			- exp3. plot reward across number of agents (is this important??) 
			- exp3. plot reward for increasing r-comm 
			- exp4. change |S| and compare runtime of sims 
			- exp5. change ni and compare runtimes 
			- plot amount of data being broadcast?? 


	prepare revision report for sjc (due wednesday):
		- comments: 
			- simulation
				- (1) more agents 
				- (2) customer waiting time reward
				- (5) baseline 
			- writing 
				- (3) not clear
				- (4) lit review
		- need his help: proof 
			- remove dependency on number of agents in consensus error 
		- show him: 
			- figs addressing (1),(2),(5)
			- gif 
			- a figure 1 for (3)
			- psuedocode for algorithms for (3)

	writeup
		- edit proof ?? oh god i hope not... ah ah ah
		- review the game theory part 

	
done: 
	3.10
		make gif tool 
		make real taxi dataset environment real taxi datasets
			- finish data write/loader
			- make env super class
			- run/debug on gridworld
			- load data from internet! 
	make macro plot tool 
	find swarm density/customer density parameter
	change agent.p to vector 
	fix customer model movement 
	refactoring
		- eliminate utilities into env 
		- change datahandler to be collection of functions that interact with high level python, and jsons and npys
		- change plotter to just a collection of functions that interacts with results folder 
	make task assignment dependent on free agent distribution (it does this already, but seems buggy)
		- check H, S, V, etc. 
	debug task assignment: agent distribution should track value func distribution (close to q value)
		- needs to be very fast!  
	debug with action vector plot
	refactor:
		- make action a state parameters 
	debug value function: each algorithm values should track the customer distribution 
		- ctd, bellman, and dtd seems fine
	make RHC baseline
	make bellman baseline
	better vis: 
		- new customer distribution at each timestep
		- compare to q value distribution at each timestep
		- compare to agent density distribution at each timestep 
		- one colorbar
	minor refactoring (step -> env)
	gradient step
		- compare against mdp calc of q at each timestep
		- check ckif
		- use plotting tool to plot q values of ckif next to dkif
		- check dkif
		- implement plotting of various sim results 
	request extension
	add decay of tau parameter in blll to force convergence
	solve mdp solution as part of training dataset 
	make mdp solver more robust/debug mdp solver error message:
	numerically stable blll 
	change R(s,a) to take into account 'a'	
	make customer demand and eta time varying with gaussians - debug
	fix value function such that game theory does what its supposed to 
	fix J calc to be local wrt actions 
	make sure 'coordinate_to_cell_index' and 'coordinate_to_xy_cell_index' are working 
	make render graphics super consistent (check that if you just drop dt problem goes away)	
	add dispatch mode
	debug dispatch mode 
	you need justification for why matching value function to agent distribution leads to a minimumizing cost fnc aka customer wait time .... has to do with optimal minimizing distribution dis


title:
~"optimal dispatch of urban taxi fleets with distributed online reinforcement learning"
	tabular (finite dimensional state/action space -> consensus methods) 
	distributed cell based mdp 
	temporal difference methods (you need to address that math paper that just came out that does D-TD...)


scratch thinking:
	how to design game theory such that local (wrt to a single agent) optimal solutions have some globally optimality propoerties (ie probabilistically converge to high-value nash equillibria)

	local marginal utility fnc where the sum of all marginal utilities gives the global potential fnc of the game 

	if you use the spatial distribution of agents, and compare with the value fnc, you obviously capture the desired trends: 
		high value fnc drives agents there
		high number of agents drive agents away

	mathematical justification?? 

	can you do better with the q value in marginal utility instead of V? 

	i feeeeeel like i should be using the local spatial distribution of the swarm, but idk  

	I think using softmax on q values is common for probabilistically choosing action. Then, if we consider the perspective of a global user, we want to pick actions according to the distribution softmax(q). So, we drive the agent distribution to the softmax(q) distribution. good news: we can calculate this locally! game over


