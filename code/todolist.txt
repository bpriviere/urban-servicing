

# clean apartment 

todo list (deadline in 3 weeks, gridworld deadline in 0 days):
	
	make fig 1 
	make gif tool 
	profile code 
	review proof

	refactoring
		- eliminate utilities into env 
		- put render in plotter 
		- clean 

	experiments
		- micro: 
			- exp1. cumulative reward for different algorithms with standard deviation plots - > running 
			- exp2. gif of evolving simulation - > data generating 
			- largescale sim -> one trial one algorithm, 1000 agents
		- macro: 
			- exp3. plot reward across number of agents (is this important??) 
			- exp3. plot reward for increasing r-comm 
			- exp4. change |S| and compare runtime of sims 
			- exp5. change ni and compare runtimes 
			- plot amount of data being broadcast?? 


	prepare revision report for sjc (due wednesday):
		- comments: 
			- simulation
				- (1) more agents 
				- (2) customer waiting time reward
				- (5) baseline 
			- writing 
				- (3) not clear
				- (4) lit review
		- need his help: proof 
			- remove dependency on number of agents in consensus error 
		- show him: 
			- figs addressing (1),(2),(5)
			- gif 
			- a figure 1 for (3)
			- psuedocode for algorithms for (3)

	develop uniform baseline

	make real taxi dataset environment real taxi datasets
		- how to encorporate traffic ?? 

	writeup
		- edit proof ?? oh god i hope not 

	nice to haves
		- distributed auction task assignment ??
	
done: 
	make macro plot tool 
	find swarm density/customer density parameter
	change agent.p to vector 
	fix customer model movement 
	refactoring
		- change datahandler to be collection of functions that interact with high level python, and jsons and npys
		- change plotter to just a collection of functions that interacts with results folder 
	make task assignment dependent on free agent distribution (it does this already, but seems buggy)
		- check H, S, V, etc. 
	debug task assignment: agent distribution should track value func distribution (close to q value)
		- needs to be very fast!  
	debug with action vector plot
	refactor:
		- make action a state parameters 
	debug value function: each algorithm values should track the customer distribution 
		- ctd, bellman, and dtd seems fine
	make RHC baseline
	make bellman baseline
	better vis: 
		- new customer distribution at each timestep
		- compare to q value distribution at each timestep
		- compare to agent density distribution at each timestep 
		- one colorbar
	minor refactoring (step -> env)
	gradient step
		- compare against mdp calc of q at each timestep
		- check ckif
		- use plotting tool to plot q values of ckif next to dkif
		- check dkif
		- implement plotting of various sim results 
	request extension
	add decay of tau parameter in blll to force convergence
	solve mdp solution as part of training dataset 
	make mdp solver more robust/debug mdp solver error message:
	numerically stable blll 
	change R(s,a) to take into account 'a'	
	make customer demand and eta time varying with gaussians - debug
	fix value function such that game theory does what its supposed to 
	fix J calc to be local wrt actions 
	make sure 'coordinate_to_cell_index' and 'coordinate_to_xy_cell_index' are working 
	make render graphics super consistent (check that if you just drop dt problem goes away)	
	add dispatch mode
	debug dispatch mode 
	you need justification for why matching value function to agent distribution leads to a minimumizing cost fnc aka customer wait time .... has to do with optimal minimizing distribution dis


title:
~"optimal dispatch of urban taxi fleets with distributed online reinforcement learning"
	tabular (finite dimensional state/action space -> consensus methods) 
	distributed cell based mdp 
	temporal difference methods (you need to address that math paper that just came out that does D-TD...)


scratch thinking:
	how to design game theory such that local (wrt to a single agent) optimal solutions have some globally optimality propoerties (ie probabilistically converge to high-value nash equillibria)

	local marginal utility fnc where the sum of all marginal utilities gives the global potential fnc of the game 

	if you use the spatial distribution of agents, and compare with the value fnc, you obviously capture the desired trends: 
		high value fnc drives agents there
		high number of agents drive agents away

	mathematical justification?? 

	can you do better with the q value in marginal utility instead of V? 

	i feeeeeel like i should be using the local spatial distribution of the swarm, but idk  

	I think using softmax on q values is common for probabilistically choosing action. Then, if we consider the perspective of a global user, we want to pick actions according to the distribution softmax(q). So, we drive the agent distribution to the softmax(q) distribution. good news: we can calculate this locally! game over


