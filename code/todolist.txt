

# charge kai for internet 
# groceries:
# 	- coffee
# 	- tacks 
# 	- ....

todo list (deadline in 3 weeks, gridworld deadline in 4 days):

	refactor:
		- make action a state parameters 

	debug with action vector plot
	
	make task assignment dependent on free agent distribution (it does this already, but seems buggy)
		- check H, S, V, etc. 

	distributed auction task assignment
	
	debug task assignment: agent distribution should track value func distribution (close to q value)
		- needs to be very fast!  

	make real taxi dataset environment real taxi datasets
		- move utility functions into enviornment 

	writeup
		- edit proof ?? oh god i hope not 
	
done: 
	debug value function: each algorithm values should track the customer distribution 
		- ctd, bellman, and dtd seems fine
	make RHC baseline
	make bellman baseline
	better vis: 
		- new customer distribution at each timestep
		- compare to q value distribution at each timestep
		- compare to agent density distribution at each timestep 
		- one colorbar
	minor refactoring (step -> env)
	gradient step
		- compare against mdp calc of q at each timestep
		- check ckif
		- use plotting tool to plot q values of ckif next to dkif
		- check dkif
		- implement plotting of various sim results 
	request extension
	add decay of tau parameter in blll to force convergence
	solve mdp solution as part of training dataset 
	make mdp solver more robust/debug mdp solver error message:
	numerically stable blll 
	change R(s,a) to take into account 'a'	
	make customer demand and eta time varying with gaussians - debug
	fix value function such that game theory does what its supposed to 
	fix J calc to be local wrt actions 
	make sure 'coordinate_to_cell_index' and 'coordinate_to_xy_cell_index' are working 
	make render graphics super consistent (check that if you just drop dt problem goes away)	
	add dispatch mode
	debug dispatch mode 
	you need justification for why matching value function to agent distribution leads to a minimumizing cost fnc aka customer wait time .... has to do with optimal minimizing distribution dis


title:
~"optimal dispatch of urban taxi fleets with distributed online reinforcement learning"
	tabular (finite dimensional state/action space -> consensus methods) 
	distributed cell based mdp 
	temporal difference methods (you need to address that math paper that just came out that does D-TD...)


scratch thinking:
how to design game theory such that local (wrt to a single agent) optimal solutions have some globally optimality propoerties (ie probabilistically converge to high-value nash equillibria)

	local marginal utility fnc where the sum of all marginal utilities gives the global potential fnc of the game 

	if you use the spatial distribution of agents, and compare with the value fnc, you obviously capture the desired trends: 
		high value fnc drives agents there
		high number of agents drive agents away

	mathematical justification?? 

	can you do better with the q value in marginal utility instead of V? 

	i feeeeeel like i should be using the local spatial distribution of the swarm, but idk  

	I think using softmax on q values is common for probabilistically choosing action. Then, if we consider the perspective of a global user, we want to pick actions according to the distribution softmax(q). So, we drive the agent distribution to the softmax(q) distribution. good news: we can calculate this locally! game over


